\documentclass[11pt, oneside]{article}  
% \documentclass[fleqn,10pt]{wlpeerj}


\usepackage{geometry}  
%\usepackage{nunito}
\usepackage{cmbright}
\geometry{letterpaper}   
%\usepackage{cite}
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex	
\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{booktabs}
% \usepackage{emoji}
\usepackage{url}
\usepackage{color}
\definecolor{c1}{rgb}{0.12, 0.56, 1.0}
%SetFonts
\usepackage{xspace}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk} 

\usepackage{sectsty} 
\definecolor{cool_blue}{RGB}{24, 132, 193}
\sectionfont{\color{c1}\selectfont}
\subsectionfont{\color{c1}\selectfont}
\subsubsectionfont{\color{c1}\selectfont}
% \paragraphfont{\color{gray}\selectfont}
\subparagraphfont{\color{gray}\selectfont}

\definecolor{fruitpushorange}{RGB}{255, 127, 0}
\newcommand{\data}{$(s_1, ..., s_k)$\xspace}

\usepackage{soul}
\DeclareRobustCommand{\cms}[1]{ {\begingroup\sethlcolor{fruitpushorange}\hl{(cms:) #1}\endgroup} }
%SetFonts


\title{A Bayesian treatment of the German tank problem}
\author[1]{Cory M. Simon}
\affil[1]{School of Chemical, Biological, and Environmental Engineering. Oregon State University. Corvallis, OR. USA. }
% \corrauthor[1]{Cory M. Simon}{cory.simon@oregonstate.edu}
\affil[]{\texttt{cory.simon@oregonstate.edu}}

% \keywords{Keyword1, Keyword2, Keyword3}



%\flushbottom
% \maketitle
%\thispagestyle{empty}


%\affil[*]{}
% \date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\begin{abstract}
	The German tank problem has an interesting historical background and is an engaging problem of statistical estimation for the classroom. The objective is to estimate the size of a population of tanks inscribed with sequential serial numbers, from a random sample. In this tutorial article, we outline the Bayesian approach to the German tank problem, (i) whose solution assigns a probability to each tank population size, thereby quantifying uncertainty, and (ii) which provides an opportunity to incorporate prior information and/or beliefs about the tank population size into the solution. We illustrate with an example. Finally, we survey problems in other contexts that resemble the German tank problem.
\end{abstract}

\clearpage

\section{Background}

\subsection{History}
To inform their military strategy during World War II (1939-1945), the Allies sought to estimate Germany's rate of production and capacity of various military equipment (tanks, tires, rockets, etc.).
Conventional methods to estimate armament production---including 
(i) extrapolating data on prewar manufacturing capabilities, 
(ii) obtaining reports from secret sources, and 
(iii) interrogating prisoners of war---were unreliable and/or contradictory. 

In 1943, British and American economic intelligence agencies exploited a German manufacturing practice in order to statistically estimate their armament production. 
Specifically, Germany marked their military equipment with serial numbers, as well as codes for the date and/or place of manufacture. Their intention was to facilitate handling spare parts and trace defective equipment/parts back to the manufacturer for quality control.
However, these serial numbers and codes on a captured sample of German equipment conveyed information to the Allies about Germany's production of it.

To estimate Germany's rate of production of tanks, the Allies collected serial numbers on the chassis, engines, gearboxes, and bogie wheels of samples of tanks by inspecting captured tanks and examining captured records\footnote{E.g., captured records from tank repair depots listed serial numbers of the chassis and engine of repaired tanks, and records from divisional headquarters listed chassis serial numbers of tanks held by a specific unit.}. 
Despite lacking an \emph{exhaustive} sample, the sequential nature\footnote{
Gearboxes on captured tanks, for example, were inscribed with serial numbers belonging to an unbroken sequence. Chassis serial numbers, on the other hand, were broken into blocks to distinguish models/designs, leaving gaps between the serial numbers assigned to them.
} of and patterns in these samples of serial numbers enabled the Allies to estimate Germany's tank production. Postwar, we know serial number analysis gave more accurate estimates than the overestimates by conventional intelligence methods (Tab.~\ref{tab:success}).
% While some components were labeled with serial numbers in discontinuous bands
% Postwar, we know serial number analysis provided more accurate estimates of German tank production than the (over)estimates from conventional American and British intelligence (Tab.~\ref{tab:success}).

See Ruggles and Brodie \cite{ruggles1947empirical} for the detailed historical account of serial number analysis to estimate German armament production during World War II.


\begin{table}[h!]
\centering 
\caption{Monthly production rate of tanks by Germany. \cite{ruggles1947empirical}} \label{tab:success}
\begin{tabular}{p{2.5cm} p{4cm} p{4cm} p{2cm}}
\toprule
 & \multicolumn{2}{c}{estimates} &   \\ 
\cmidrule(r){2-3}
month & conventional American \& British Intelligence & serial number analysis  & German records \\
\midrule
June, 1940 & 1000& 169 &  122 \\
June, 1941 &1550 & 244 &   271 \\
August, 1942 & 1550& 327  & 342 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{The German tank problem}
Simplification of the historical context to estimate German tank production via serial number analysis \cite{ruggles1947empirical} motivated the formulation of the textbook-friendly \emph{German tank problem} \cite{goodman1952serial}: 
\begin{tcolorbox}[title=Problem statement, colback=white, colframe=c1]
\noindent 
During World War II, the German military is equipped with $n$ tanks. 
Each tank is inscribed with a unique serial number in $\{1, ..., n\}$. \\

As the Allies, we do not know $n$, but we captured (without replacement, of course) a sample of $k$ German tanks with (ordered) inscribed serial numbers \data. 

\begin{center}
	\begin{tabular}{cccc}
		\includegraphics[width=0.125\textwidth]{tank.png} &  \includegraphics[width=0.125\textwidth]{tank.png}  & & \includegraphics[width=0.125\textwidth]{tank.png} \\
		\large $s_1$ & \large $s_2$ & \large $\cdots$ & \large $s_k$ 
	\end{tabular}
\end{center}

Assuming (i) all tanks in the population were equally likely to be captured and (ii) $n$ was/is fixed, our objective is to estimate $n$ in light of the data \data.
%Find an estimator $\mathcal{T}: \{s_1, s_2, ..., s_k\} \mapsto \hat{n}$ that maps the set of observed serial numbers on the captured tanks to an estimate of the number of German tanks, $\hat{n}$.
% Assume $n$ is fixed over the time scale of capturing the tanks.
\end{tcolorbox}

% While speculation that  the earliest variant of the German tank problem \cite{hall2014alan}
In 1942, Alan Turing and Andrew Gleason discussed a variant of the German tank problem, ``how to best to estimate the total number of taxicabs in a town, having seen a random selection of their license numbers'', in a crowded restaurant in Washington DC \cite{hodges2014alan,hall2014alan}. 
Today, with its interesting historical background \cite{ruggles1947empirical}, 
the German tank problem is still a suitable conversation topic for dinners and serves as an intellectually engaging, challenging, and enjoyable problem to illustrate combinatorics and statistical estimation in the classroom \cite{johnson1994estimating,berg2021bayesian,mosteller1987fifty,downey2021think}.

\paragraph{Uncertainty quantification.}
Any estimate of the tank population size $n$ from the data \data is subject to uncertainty, since we (presumably) have not captured \emph{all} of the tanks (i.e., $k\neq n$, probably).  
Quantifying uncertainty in our estimate of $n$ is important because high-stakes military decisions may be made on the basis of it.

\paragraph{Our contribution.}
In this pedagogical article, we outline the Bayesian approach to the German tank problem, 
(i) whose solution assigns a probability to each tank population size, thereby quantifying uncertainty, and
(ii) which provides an opportunity to incorporate prior information and/or beliefs about the tank population size into the solution.


\subsection{Survey of previous work on the German tank problem}
\paragraph{The frequentist approach.}
Border \cite{bordernotes} calls the German tank problem a ``weird case'' in frequentist estimation. The maximum likelihood estimator of the tank population size $n$ is the maximum serial number observed among the $k$ captured tanks, $m^{(k)}:=\max_{i \in \{1, ..., k\}} s_i$. This is a biased estimator, as certainly $m^{(k)} \leq n$.

Goodman \cite{goodman1952serial,goodman1954some} derives the minimum-variance, unbiased estimator of the tank population size
\begin{equation}
	\hat{n} = m^{(k)} + \left(\frac{m^{(k)}}{k}-1 \right). \label{eq:nhat}
\end{equation}
To intuit $\hat{n}$, note (i) $n$ must be greater than or equal to $m^{(k)}$ and (ii) if we observe large (small) gaps between the serial numbers \data after sorting them (incl.\ the gap preceding the smallest serial number), then $n$ is likely (unlikely) to be much greater than $m^{(k)}$. 
The estimator of $n$ in eqn.~\ref{eq:nhat} quantifies how far beyond the maximum serial number $m^{(k)}$ we should estimate the tank population size, based on the gaps; $m^{(k)}/k-1$ is the average size of the gaps.
% between the sorted serial numbers (including the gap preceding the smallest serial number).
Goodman \cite{goodman1952serial} also derives a frequentist, two-sided $1-a$ confidence interval for $n$: $m^{(k)}\leq n \leq x$, where $x$ is the largest integer satisfying $(m^{(k)}-1)_{k} /(x)_k \geq a$ with notation for the falling factorial defined in eqn.~\ref{eq:falling_factorial}.

\paragraph{For pedagogy.} Champkin highlights the application of statistics to estimate German tank production during WWII as a "great moment in statistics" \cite{grajalez2013great}. 
Johnson lists and evaluates several intuitive point estimators for the size of the tank population \cite{johnson1994estimating}. 
Scheaffer, Watkins, Gnanadesikan, and Witmer \cite{scheaffer2013activity} propose a hands-on learning activity to illustrate the German tank problem by sampling chips, labeled with numbers from 1 to $n$, from a bowl. 
Inspired by the German tank problem, Berg \cite{berg2021bayesian} orchestrates a classroom-based competition to best-estimate the size of a population of a city from a random sample. Clark, Gonye, and Miller explore using simulations of tank capturing and linear regression to discover the estimator in eqn.~\ref{eq:nhat} \cite{clark2021lessons}. 


\paragraph{The Bayesian approach.}
Closely related to our pedagogical exploration of the Bayesian approach to the German tank problem, Roberts \cite{roberts1967informative}, H{\"o}hle and Held \cite{hohle2006bayesian}, and Linden, Dose, and Toussaint \cite{von2014bayesian}, and Cocco, Monasson, and Zamponi \cite{cocco2022statistical} undertake a Bayesian analysis of the German tank problem and provide an analytical formula for the mean and variance of the posterior distribution of the tank population size under an improper, uniform prior distribution. Andrews \cite{blogpost} outlines the Bayesian approach to the German tank problem in a blog post containing code in the R language. 
Rosenberg and Deely \cite{rosenberg1976horse} outline an empirical Bayes approach to estimate the number of horses in a race from a sample of numbered horses (the likelihood function here is equivalent to that in the German tank problem). 
Berg and Hawila \cite{berg2021bayesian2} use Bayesian inference for the closely related taxicab problem. 

\paragraph{Generalizations/variants.}
Goodman \cite{goodman1952serial,goodman1954some} and Clark, Gonye, and Miller \cite{clark2021lessons} pose a variant of the German tank problem where the initial serial number is not known; i.e., where the $n$ tanks are inscribed with serial numbers $\{b+1, ..., n+b\}$ with $b$ and $n$ unknown. 
Lee and Miller generalize the German tank problem to the settings where the serial numbers belong to a continuum and/or lie in two or higher dimensions, within a square or circle \cite{lee2022generalizing}. 
% with replacement?

\subsection{Overview of the Bayesian approach to the German tank problem}
Adopting a Bayesian perspective \cite{downey2021think,bolstad2016introduction,van2021bayesian}, we treat the (unknown) total number of tanks as a discrete random variable $N$ to model our uncertainty about it. A probability mass function of $N$ assigns a probability to each possible tank population size $n$. This probability is a measure of our degree of belief, perhaps with some basis in knowledge/data, that the tank population size is $n$ \cite{ghosh2006introduction}. The spread of the mass function of $N$ over the integers reflects uncertainty. 

The observed serial numbers \data convey information about the tank population size. Hence, the probability mass function of $N$ changes after the data \data are collected and considered. I.e., $N$ has a \emph{prior} and \emph{posterior} probability mass function.

The three inputs to a Bayesian treatment of the German tank problem are: 
\begin{enumerate}
	\item the \emph{prior} mass function of $N$, which expresses a combination of our subjective beliefs and objective knowledge about the tank population size before we collect and consider the sample of serial numbers.
	\item the \emph{data}, the sample of serial numbers \data, viewed as realizations of random variables $(S_1, ..., S_k)$ owing to the stochasticity of tank-capturing.
	\item the \emph{likelihood} function, giving the probability of the data $(S_1, ..., S_k)=(s_1,...,s_k)$ under each tank population size $N=n$, based on a probabilistic model of the tank-capturing process.
\end{enumerate}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.85\textwidth]{gtp_fig0.pdf} 
	\caption{Bayes' theorem applied to the German tank problem. 
	An Euler diagram \cite{ottley2012visually,micallef2012assessing} represents the two events $S^{(k)}=s^{(k)}$ and $N=n$ with circles. The area of each circle is proportional to the probability of the event, and the area of overlap is proportional to the probability of the intersection of the events, $(S^{(k)}=s^{(k)}) \cap (N=n)$.
	The Euler diagram rationalizes the two statements of conditional probability in terms of the intersection of the events, which in turn imply Bayes' theorem. \cite{kruschke2014doing}
	} \label{fig:bayes}
\end{figure}

The output of a Bayesian treatment of the German tank problem is the \emph{posterior} mass function of $N$, conditioned on the data \data. The posterior follows from Bayes' theorem and can be viewed as an update to the prior in light of the data. Fig.~\ref{fig:bayes} illustrates.
The posterior mass function of $N$ assigns each possible tank population size $n$ with a probability according to a compromise between its
% based on its consistency with (i) the data \data, according to the likelihood function, and (ii) our prior beliefs about the size of the tank population, expressed by the prior mass function. 
% I.e., the posterior is a compromise between 
(i) likelihood, which invokes the probabilistic tank-capturing model to quantify the support lended by the observed serial numbers \data for the hypothesis that the tank population size is $n$, 
and 
(ii) prior probability, which quantifies how likely we thought the tank population size might be $n$ before the serial numbers \data were collected and considered. \cite{van2021bayesian} 
The posterior mass function of $N$ is the raw Bayesian solution to the German tank problem; its spread quantifies our posterior uncertainty about $N$. We may summarize the posterior by reporting (1) its median and (2) a small subset of the integers on which most of the posterior mass sits---a \emph{credible set} that likely contains the tank population size. More, from the posterior, we can answer questions such as, "what is the probability that $N$ exceeds some threshold quantity $n^\prime$ that would alter military strategy?".
%We specify the \emph{prior} probability mass function of $N$ to express our beliefs about the tank population size before the serial numbers are collected and considered. The prior may be flat and diffuse if we have no prior information, or it could concentrate its mass around some estimate obtained by another means.


% The raw, uncertainty-quantifying solution to the German tank problem under a Bayesian approach is the posterior probability mass function of the size of the tank population, $N$. 

\section{The Bayesian approach to the German tank problem}
We now delve into the details of the Bayesian approach to the German tank problem and illustrate via an example. 
%We now tackle the German tank problem from a Bayesian standpoint. 
For reference, the variables are listed in Tab.~\ref{tab:params}. 
We use upper- and lower-case letters to represent random variables and realizations of them, respectively. 
Throughout, we employ the indicator function associated with a set $A$:
\begin{equation}
\mathcal{I}_{A}(x) = \begin{cases} 1 & x \in A \\ 0 & x \notin A . \end{cases} 
\end{equation}
%$$ which maps its input $x$ to 1 if $x$ belongs to the set $A$ and to 0 otherwise (i.e., if $x\notin A$).

\begin{table}[h!]
	\centering
	\caption{List of parameters/variables.} \label{tab:params}
	\begin{tabular}{c c l}
		\toprule
		parameter/variable & $\in$ & description \\
		\midrule
		$n$ & $\mathbb{N}_{\geq 0}$ & size of population of tanks \\
		$k$ & $\mathbb{N}_{>0}$ &  number of captured tanks \\
		$s_i$ & $\mathbb{N}_{>0}$ &  serial number on captured tank $i$ \\
		$s^{(k)}$ & $\mathbb{N}_{>0}^k$ & vector listing the serial numbers on the $k$ captured tanks \\
		$m^{(k)}$ & $\mathbb{N}_{>0}$ &  maximum serial number among the $k$ captured tanks \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{The prior distribution}
We construct the \emph{prior probability mass function} $\pi_{\text{prior}}(N=n)$ to express a combination of our subjective beliefs and objective knowledge about the total number of tanks $N$ before the data \data are collected and considered. 

The prior mass function we appropriately impose on $N$ depends on the context. 
If we do not possess prior information about the tank population size, we may adopt the principle of indifference and impose a \emph{diffuse} prior, e.g.\ a uniform distribution over a set of feasible tank population sizes. 
On the other hand, if we possess a rough estimate of the number of tanks from some other source of information or analysis, we may construct a more \emph{informative} prior that concentrates its mass around this estimate. 
By definition, a diffuse prior admits more uncertainty (measured by e.g.\ entropy \cite{murphy2022probabilistic}) about the tank population size than a more informative prior \cite{van2021bayesian}.

Thinking ahead about the posterior mass function of $N$, which balances the prior and the likelihood (the latter based on the data): 
(1) a more informative prior will have a larger impact on the posterior than a diffuse one \cite{van2021bayesian}, which "lets the data speak for itself" \cite{downey2021think};
(2) generally, as the number of captured tanks $k$ increases, we expect the prior to have a smaller impact on the posterior \cite{downey2021think} as the data "overwhelms" the prior.


\subsection{The data, data-generating process, and likelihood function}
\paragraph{The data.} The data we obtain in the German tank problem is the vector of serial numbers inscribed on the $k$ captured tanks
\begin{equation}
	s^{(k)}:=(s_1,...,s_k).
\end{equation} 
% At this point, we entertain the possibility that order matters, so $s^{(k)}$ is a vector whose entry $i$ is the serial number $s_i$ of the $i$th tank captured. 
We view the data $s^{(k)}$ as a realization of the discrete random vector $S^{(k)}:=(S_1, ..., S_k)$. At this point, we are entertaining the possibility that the order in which tanks are captured matters.

\paragraph{The data-generating process.}
The stochastic \emph{data-generating process} constitutes sequential capture of $k$ tanks from a population of $n$ tanks, without replacement, then inspecting their serial numbers to construct $s^{(k)}$.
We assume that each tank in the population is equally likely to be captured at each step.
Then, mathematically, the stochastic data-generating process is sequential, uniform random selection of $k$ integers, without replacement, from the set $\{1, ..., n\}$.

\paragraph{The likelihood function.}
The \emph{likelihood function} specifies the probability of the data $S^{(k)}=s^{(k)}$ given each tank population size $N=n$.
% There are $\binom{n}{k}$ ("$n$ choose $k$") subsets of $\{1, ..., n\}$ that contain $k$ distinct integers. 
Each outcome $s^{(k)}$ in the sample space $\Omega_n^{(k)}$ is equally likely, where
\begin{equation}
	\Omega_n^{(k)} := \{ (s_1, ..., s_k)_{\neq}  :  s_i \in \{1, ..., n\} \; \text{for all } i \in \{ 1,..., k \} \},
\end{equation} with $(\cdots)_{\neq}$ meaning the elements of the vector $(\cdots)$ are unique. 
The number of outcomes in the sample space, $|\Omega_n^{(k)}|$, is the number of distinct ordered arrangements of $k$ distinct integers from the set $\{1,...,n\}$, given by the falling factorial:
\begin{equation}
	(n)_k:= n(n-1)\cdots (n-k+1) = \frac{n!}{(n-k)!}. \label{eq:falling_factorial}
\end{equation}
Under the data-generating process, then, the probability of observing data $S^{(k)}=s^{(k)}$ given the tank population size $N=n$ is the uniform distribution:
\begin{equation}
	\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)=
	\dfrac{1}{(n)_k} \mathcal{I}_{\Omega_n^{(k)}}\left(s^{(k)}\right).
	\label{eq:dgp}
\end{equation}

\subparagraph{Interpretation.}
The likelihood quantifies the support lended by the serial numbers on the $k$ captured tanks in $s^{(k)}$, when compared with our probabilistic model of the tank-capturing process, for the hypothesis that the tank population size is $n$ \cite{van2021bayesian}. We view $\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)$ as a function of $n$, since in practice we possess the data $s^{(k)}$ but not $n$.

\subparagraph{The likelihood as a sequence of events.} 
Alternatively, we may arrive at eqn.~\ref{eq:dgp} from a perspective of sequential events $S_1=s_1, S_2=s_2, ..., S_k=s_k$. First, the probability of a given serial number on the $i$th captured tank, conditioned on the tank population size and the serial numbers on the previously captured tanks, is the uniform distribution
\begin{equation}
	\pi (S_i=s_i \mid N=n, S_1=s_1, ..., S_{i-1}=s_{i-1})=\dfrac{1}{n-i+1} \mathcal{I}_{ \{1,...,n\} \setminus \{s_1, ..., s_{i-1}\}}(s_i)
\end{equation}
since there are $n-(i-1)$ tanks to choose from at uniform random.
By the chain rule of probability \cite{koch2007introduction}, the joint probability
\begin{equation}
	\pi_{\text{likelihood}}(S_1=s_1, ..., S_k=s_k \mid N=n) = \displaystyle \prod_{i=1}^k \pi (S_i=s_i \mid N=n, S_1=s_1,...,S_{i-1}=s_{i-1})
	%  \\ \pi(S_1=s_1 \mid N=n) \pi(S_2=s_2 \mid N=n, S_1=s_1) \cdots \\ \pi(S_k=s_k \mid N=n, S_1=s_1, S_2=s_2, ..., S_{k-1}=s_{k-1}), 
\end{equation}
giving eqn.~\ref{eq:dgp} after simplifying the product of indicator functions.

\subparagraph{The likelihood function in terms of the maximum observed serial number.}
We will find in Sec.~\ref{sec:posterior} that only two independent features of the data \data provide information about the tank population size, $N$: its (i) size, $k$, and (ii) maximum observed serial number 
\begin{equation}
    m^{(k)} = \max_{i \in \{1, ..., k\}} s_i .
\end{equation} 
Thus, we also write a different likelihood: the probability of observing a maximum serial number $m^{(k)}$ given the tank population size $N=n$,  $\pi_{\text{likelihood}}(M^{(k)}=m^{(k)} \mid N=n)$.

Because each outcome $s^{(k)}\in \Omega_n^{(k)}$ is equally likely, $\pi_{\text{likelihood}}(M^{(k)}=m^{(k)} \mid N=n)$ is the fraction of sample space $\Omega_n^{(k)}$ where the maximum serial number is $m^{(k)}$.
%        \begin{equation}
%        	\pi_{\text{likelihood}}(M^{(k)}=m^{(k)} \mid N=n) =
%        	 \dfrac{\lvert \{ (s_1, ..., s_k) \in \Omega_n^{(k)} : \max_{i \in \{1, ...,k\}} s_i = m^{(k)}\}\rvert}{\lvert \Omega_n^{(k)} \rvert}
%        	 \label{eq:likelihood_m_start}
%        \end{equation}
To count the outcomes $s^{(k)}\in\Omega_n^{(k)}$ where the maximum serial number is $m^{(k)}$, consider (i) one of the $k$ captured tanks has serial number $m^{(k)}$ and (ii) the remaining $k-1$ tanks have a serial number in $\{1, ..., m^{(k)}-1\}$.
For each of the $k$ possible positions of the maximum serial number in the vector $s^{(k)}$, there are $(m^{(k)}-1)_{k-1}$ distinct outcomes specifying the other $k-1$ entries.
%(the number of "$(k-1)$-permutations of $m-1$ objects without repetition"). 
% Then, eqn.~\ref{eq:likelihood_m_start} becomes:
Thus:
\begin{equation}
	\pi_{\text{likelihood}}(M^{(k)}=m^{(k)} \mid N=n)=
	\dfrac{k(m^{(k)}-1)_{k-1}}{(n)_k} \mathcal{I}_{\{k,...,n\}}(m^{(k)}). \label{eq:likelihood_m}
 \end{equation}
 

\subsection{The posterior distribution}
\label{sec:posterior}
The \emph{posterior probability mass function} of $N$ assigns a probability to each possible tank population size $n$ in consideration of its consistency with (1) the data \data, according to the likelihood in eqn.~\ref{eq:dgp}, and (2) our prior beliefs/knowledge encoded in $\pi_{\text{prior}}(N=n)$. 

The posterior distribution is a conditional distribution related to the likelihood and prior mass functions by Bayes' theorem \cite{kruschke2014doing} (see Fig.~\ref{fig:bayes}):
\begin{equation}
	\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)}) = 
	\frac{\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n) \pi_{\text{prior}}(N=n)}{\pi_{\text{evidence}}(S^{(k)}=s^{(k)})}. \label{eq:post}
\end{equation} 
The denominator, the \emph{evidence} \cite{kruschke2014doing}, is the probability of the data $s^{(k)}$:
\begin{equation}
	\pi_{\text{evidence}}(S^{(k)}=s^{(k)})= \displaystyle \sum_{n^\prime=0}^\infty \pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n^\prime) \pi_{\text{prior}}(N=n^\prime). \label{eq:prob_data}
\end{equation}
We view $\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)})$ as a probability mass function of $N$, since in practice we have $s^{(k)}$. 
Then, $\pi_{\text{evidence}}(S^{(k)}=s^{(k)})$, which is independent of $n$, is just a normalizing factor for the numerator in eqn.~\ref{eq:post}. 

Interpreting eqn.~\ref{eq:post}, the prior mass function of $N$ is \emph{updated}, in light of the data \data, to yield the posterior mass function of $N$. The posterior probability that $N=n$ is proportional to the product of the likelihood at and prior probability of $N=n$, giving a compromise between the likelihood and prior.

We simplify the posterior mass function of $N$ in eqn.~\ref{eq:post} by (i) substituting eqn.~\ref{eq:dgp}, (ii) restricting the sum in eqn.~\ref{eq:prob_data} to tank population sizes where the likelihood is nonzero, and (iii) noting the only two features of the data \data that appear are (a) its size $k$ and (b) the maximum serial number $m^{(k)}$:
\begin{align}
	\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)}) &= 
	\pi_{\text{posterior}}(N=n \mid M^{(k)}=m^{(k)})  \nonumber \\
	& = 
	\frac{
		\displaystyle (n)_k^{-1} \pi_{\text{prior}}(N=n)
	}{
		\displaystyle \sum_{n^\prime=m^{(k)}}^\infty (n^\prime)_{k}^{-1}  \pi_{\text{prior}}(N=n^\prime)
	}
	\mathcal{I}_{\{m^{(k)}, m^{(k)}+1,...\}}(n)
	\label{eq:post_simple}
\end{align}
% We may write the posterior distribution $\pi_{\text{posterior}}(N=n \mid S^{(k)}=\{s_1, ..., s_k\})$ more simply as $\pi_{\text{posterior}}(N=n \mid M^{(k)}=m^{(k)})$.
% i.e., the only two properties of the data \data that provide information about the tank population size $N$ are: (i) the size of the data, $k$, and (ii) the maximum serial number, $m^{(k)}$. 
Note, we may arrive at eqn.~\ref{eq:post_simple} through eqn.~\ref{eq:likelihood_m} as well.
% To see this, suppose we have two distinct outcomes $s^{(k)}=\{s_1, ..., s_k\}$ and $s^{\prime (k)}=\{s_1^\prime, ..., s_k^\prime\}$ with the same maximum serial number, $m^{(k)}=\max_i s_i = \max_i s_i^\prime$. Note $s^{(k)}\in \mathcal{V}_n \iff s^{\prime (k)}\in\mathcal{V}_n$ for $n\in\mathbb{Z}_{>0}$. Thus, $\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)= \pi_{\text{likelihood}}(S^{(k)}=s^{\prime (k)} \mid N=n)$ for $n\in \mathbb{Z}_{>0}$ and, via eqn~\ref{eq:post}, $\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)}) =\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{\prime (k)}) $ for $n\in \mathbb{Z}_{>0}$.

\paragraph{Interpretation.}
The posterior probability mass function of $N$ in eqn.~\ref{eq:post_simple} assigns a probability to each tank population size $n$ in consideration of the serial numbers \data observed on the captured tanks, our probabilistic model of the tank-capturing process, and our prior beliefs and knowledge about the tank population size expressed in the prior mass function of $N$. The spread (measured e.g.\ by entropy) of the posterior mass function of $N$ reflects remaining epistemic (reducible with more data) \cite{fox2011distinguishing,senge2014reliable} uncertainty about the tank population size.

% \cms{should I introduce PMF for probability mass function?}

\paragraph{A remark on "uncertainty".}
The source of posterior uncertainty is a lack of complete data: we have not captured all of the tanks\footnote{
Certainly, $k<n$ if there are gaps in the observed serial numbers \data. Even if there are no gaps in \data, we cannot be certain we have captured the tank with the largest serial number.
} and observed their serial numbers to be certain of the tank population size.
In practice, an additional source of posterior uncertainty about the tank population size is the possible inadequacy of the model of the tank-capturing process (uniform sampling) in eqn.~\ref{eq:dgp}. I.e., selection bias could be present in the tank-capturing process. Our analysis here neglects this source of uncertainty. 

\paragraph{Summarizing the posterior mass function of $N$.}
We may summarize the posterior mass function of $N$ with a point estimate of the tank population size and a credible subset of the integers that contains the tank population size with a high probability\footnote{Well, under our assumptions embedded in the likelihood and prior mass functions.}.
A suitable point estimate of the tank population size is a median of the posterior mass function of $N$; by definition, the posterior probability that the tank population size is greater (less) than or equal to a median is at least 0.5.
A suitable credible subset, which entertains multiple tank population sizes, is the $a$-high-mass subset \cite{hyndman1996computing}
\begin{equation}
	\mathcal{H}_a:= \{n^\prime : \pi_{\text{posterior}}(N=n^\prime \mid M^{(k)}=m^{(k)}) \geq \pi_a\}
\end{equation} where $\pi_a$ is the largest mass to satisfy 
\begin{equation}
	\pi_{\text{posterior}}(N \in \mathcal{H}_a \mid M^{(k)}=m^{(k)}) \geq 1 - a.
\end{equation}
In words, the $a$-high-mass subset $\mathcal{H}_a$ is the smallest to (i) contain at least a fraction $1-a$ of the posterior mass of $N$ and (ii) ensure every tank population size belonging to it is more probable than any outside of it. 
% Conceptually, we may think of $\pi_a$ as a water level that we raise to submerge tank population sizes that have low posterior probability.

\paragraph{Querying the posterior distribution.} We may find the posterior probability that the tank population size belongs to any set of interest by summing the posterior mass over it. E.g., the probability the tank population size exceeds some number $n^\prime$ is:
\begin{equation}
	\pi_{\text{posterior}}(N> n^\prime \mid M^{(k)}=m^{(k)}) = \sum_{n=n^\prime+1}^\infty \pi_{\text{posterior}}(N=n \mid M^{(k)}=m^{(k)}).
\end{equation}

%\subsubsection{Posterior predictive checking}
%We wish to check the consistency of the data $s^{(k)}$ with the posterior mass function of $N$.
%Conceptually, we can simulate new data $\tilde{s}^{(k)}$ using the model of the tank-capturing process under a sample of the tank population size from the posterior, then compare the simulated data $\tilde{s}^{(k)}$ to the real data $s^{(k)}$ \cite{https://doi.org/10.1111/rssa.12378,van2021bayesian}. 
%The probability that serial number $\tilde{s}$ would be captured in such a simulation is:
%\begin{equation}
%	\pi(\tilde{s} \in \tilde{S}^{(k)}) = \sum_{n^\prime=k}^\infty \frac{k}{n^\prime}\pi_{\text{posterior}}(N = n^\prime \mid S^{(k)}=s^{(k)}) \mathcal{I}_{\{1,...,n^\prime\}}(\tilde{s}), \label{eq:posterior_check}
%\end{equation} since $k/n^\prime$ is the probability any given viable serial number $\tilde{s}$ will be observed given the tank population size $N=n^\prime$.
%We then compare the serial numbers in the real data \data with $\pi(\tilde{s} \in \tilde{S}^{(k)})$.
%% Two other summaries take more advantage of the Bayesian approach.



\section{Example}
We illustrate the Bayesian approach to the German tank problem through an example.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{prior.pdf} \caption{prior mass function of $N$} \label{fig:the_prior}
	\end{subfigure} 
	
	\begin{subfigure}[b]{0.5\textwidth}
%	\begin{tabular}{ccccc}
%		\includegraphics[width=0.135\textwidth]{tank.png} &  \includegraphics[width=0.135\textwidth]{tank.png}  &   \includegraphics[width=0.135\textwidth]{tank.png}  & \includegraphics[width=0.135\textwidth]{tank.png} &   \includegraphics[width=0.135\textwidth]{tank.png}  \\
%		$s_1=12$ &  $s_2=10$ & $s_3=6$ & $s_4=11$ & $s_5=15$ 
%	\end{tabular} 
		\includegraphics[width=\textwidth]{the_sample.pdf} 
	\caption{the data $s^{(k=3)}$} \label{fig:the_data}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{likelihood.pdf} \caption{the likelihood function} 		
		\label{fig:the_likelihood}
	\end{subfigure}

	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{posterior_35.pdf} \caption{posterior mass function of $N$} \label{fig:the_posterior}
	\end{subfigure}
	\caption{A Bayesian approach to the German tank problem. 
	(a, prior) The prior mass function.
	(b, data) The data $s^{(3)}$, with maximum observed serial number $m^{(3)}=15$.
	(c, likelihood) The likelihood function associated with the data $s^{(3)}$.
	(d, posterior) The posterior mass function of $N$. $\mathcal{H}_{0.2}$ highlighted; median marked with vertical, dashed line.
	}
\end{figure}

\paragraph{The prior probability mass function of $N$.}
Suppose we have an upper bound $n_{\text{max}}$ for the possible number of tanks, based on e.g.\ the supply of some raw material needed for tank production, but no other information. Then, we may impose a diffuse prior, a uniform prior probability mass function:
\begin{equation}
	\pi_{\text{prior}}(N=n) = \dfrac{1}{n_{\max}+1} \mathcal{I}_{ \{0, ..., n_{\text{max}}\}}(n).	 \label{eq:prior}
\end{equation} 
This prior mass function expresses: in the absence of any data \data (i.e., no serial numbers, not $k$ either), we believe the total number of tanks $N$ is equally likely to be a value in $\{0, ..., n_{\text{max}}\}$. Particularly, suppose $n_\text{max}=35$. Fig.~\ref{fig:the_prior} visualizes $\pi_{\text{prior}}(N=n)$.

\paragraph{The data \data and likelihood function.} Now suppose we capture $k=3$ tanks, with serial numbers $s^{(3)}=(15, 14, 3)$. See Fig.~\ref{fig:the_data}. So, the maximum observed serial number is $m^{(3)}=15$. 
The likelihood function $\pi_{\text{likelihood}}(M^{(3)}=15 \mid N=n)$ in eqn.~\ref{eq:likelihood_m} is displayed in Fig.~\ref{fig:the_likelihood}. Note, the likelihood function is maximal at $n=m^{(3)}=15$ and monotonically decreases. 

\paragraph{The posterior probability mass function of $N$.}
Under the uniform prior in eqn.~\ref{eq:prior}, the posterior probability mass function of $N$ in eqn.~\ref{eq:post_simple} becomes:
\begin{equation}
	\pi_{\text{posterior}}(N=n \mid M^{(k)}=m^{(k)})= 
		\dfrac{
			(n)_k^{-1}
			}{
			\displaystyle \sum_{ n^\prime = m^{(k)}}^{n_{\text{max}}} (n^\prime)_k^{-1}
		} \mathcal{I}_{ \{m^{(k)}, m^{(k)}+1, ..., n_\text{max}\} }(n).
\end{equation}
Fig.~\ref{fig:the_posterior} visualizes the posterior probability mass function of $N$ for the data $s^{(3)}$ in Fig.~\ref{fig:the_data} and the prior in eqn.~\ref{eq:prior} ($n_\text{max}=35$). 

\subparagraph{Summarizing the posterior.}
Summarizing the posterior mass function of $N$, its median is $19$ and its high-mass credible subset $\mathcal{H}_{0.2}=\{15, ..., 25\}$ (highlighted in Fig.~\ref{fig:the_posterior}). 
For what it's worth, the data in Fig.~\ref{fig:the_data} was generated from a tank population size of $n=20$ (explaining the choice of scale in Fig.~\ref{fig:the_data}). 

\subparagraph{Querying the posterior.} Suppose our military strategy would change if the size of the tank population were to exceed 30. From the posterior distribution of $N$, we calculate $\pi_{\text{posterior}}(N>30 \mid M^{(3)}=15)\approx 0.066$.

%\subparagraph{Posterior predictive checking.} As a posterior predictive check, Fig.~\ref{fig:posterior_checking} shows how the observed serial numbers in the data $s^{(3)}$ compare with the probability of observing each serial number under the posterior mass function of $N$, according to eqn.~\ref{eq:posterior_check}.

\paragraph{Sensitivity of the posterior to the prior.} Because of the subjectivity involved in constructing the prior, checking the sensitivity of the posterior to the prior is good practice \cite{van2021bayesian}. Fig.~\ref{fig:posterior_sensitivity} shows how the posterior mass function of $N$ changes with the upper-bound on the tank population $n_\text{max}$ we impose via the prior mass function of $N$ in eqn.~\ref{eq:prior}. 
E.g., under $n_\text{max} =75$, the high-mass subset $\mathcal{H}_{0.2}$ expands to $\{15, ..., 29\}$.
% Thus, our posterior point estimate and credible interval for the tank population size is not very sensitive to the choice of the upper bound $n_\text{max}$ in the prior.

% interpret posterior more.
  \begin{figure}[h!]
        	\centering
%	\begin{subfigure}[b]{0.41\textwidth}
%        		\includegraphics[width=\textwidth]{posterior_check.pdf}
%        		\caption{posterior predictive check} \label{fig:posterior_checking}
%        	\end{subfigure}
	
%    	\begin{subfigure}[b]{\textwidth}
%		\centering
		\includegraphics[width=0.75\textwidth]{posterior_prior_sensitivity.pdf}
%        		\caption{sensitivity of the posterior to the prior} 
%        	\end{subfigure} 
            \caption{Evaluating the sensitivity of the posterior mass function of $N$ to the upper bound $n_{\text{max}}$ imposed by the prior mass function of $N$.} \label{fig:posterior_sensitivity}
\end{figure}

\paragraph{Capturing more tanks.} Suppose we capture an additional 9 tanks and re-run the Bayesian analysis. Fig.~\ref{fig:moretanks} shows the updated posterior mass function of $N$. The high-mass credible subset $\mathcal{H}_{0.2}$ shrinks considerably, to $\{19, 20\}$. This shows how more data---increasing the number of tanks captured, $k$---generally reduces our uncertainty about the tank population size.

  \begin{figure}[h!]
        	\centering
	\begin{subfigure}[b]{0.5\textwidth}
        		\includegraphics[width=\textwidth]{the_big_sample.pdf}
		\caption{the updated data $s^{(k=12)}$}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
	 \includegraphics[width=\textwidth]{posterior_big_35.pdf}
	 \caption{the updated posterior mass function of $N$}
	     	\end{subfigure}
            \caption{The posterior distribution of $N$ after we capture more tanks. (a) We capture an additional 9 tanks. (b) The updated posterior mass function of $N$.
            	} \label{fig:moretanks}
\end{figure}

\section{Simulations to investigate the behavior of the posterior of $N$ under a known population size}
% \paragraph{The average high-mass subset and median median of the posterior over tank-capturing under a fixed tank population size.}
\label{sec:sims}
We now investigate how, on average, over the stochastic outcomes of the tank-capturing process for a fixed tank population size, the posterior distribution $\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)})$ depends on (i) the number of tanks captured, $k$, and (ii) the maximum of the support of the uniform prior, $n_{\text{max}}$.

For a given $k$ and $n_{\text{max}}$, we conduct 50\,000 simulations, each constituting (i) capturing $k$ random tanks from a population of $n=20$ tanks, giving data $s^{(k)}$, (ii) computing the posterior mass function $\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)})$, then (iii) finding the high-mass subset $\mathcal{H}_a$ ($a=0.2$) and median of the posterior. Fig.~\ref{fig:H_a_study} displays (1, stems) the probability of each tank population size $n$ belonging to $\mathcal{H}_a$ and (2, vertical line) the median of the median of the posterior, for $(k, n_{\text{max}})\in \{3, 6, 9\} \times \{25, 50, 100\}$.
As $k$ increases, the high-mass subset $\mathcal{H}_a$ tends to (a) be less sensitive to $n_{\text{max}}$, as the data overrides the prior, and (b) shrink, as uncertainty decreases with a larger sample. As $n_{\text{max}}$ increases, larger population sizes become more likely to be included in $\mathcal{H}_a$.
The median of the median of the posterior matches the true tank population size of 20 when $n_{\text{max}}=25$ or $k=9$. For $k\in\{3, 6\}$, the larger $n_{\text{max}}$ pull the median to the right of the true tank population size.
 
  \begin{figure}[h!]
        	\centering
        	\includegraphics[width=\textwidth]{gtp_fig4.pdf}
            \caption{
            The average high-mass subset and median median of the posterior over tank-capturing under a fixed tank population size. 
            Rows: different \#'s of tanks captured, $k$ (left).
            Columns: different maxima of tank population sizes entertained by the uniform prior, $n_{\text{max}}$ (top). 
            For a particular $(k, n_{\text{max}})$, the stem plots show the probability of each tank population size belonging to the high-mass subset $\mathcal{H}_{a=0.2}$ of the posterior. The vertical dashed line shows the median of the median of the posterior.
            Red arrow shows the true tank population size, 20. 
            	} \label{fig:H_a_study}
\end{figure}

\section{Discussion}
\paragraph{Selection bias.}
A strict assumption in the textbook-friendly German tank problem, which enables us to estimate the size of the population of tanks from a random sample of their (sequential) serial numbers, is that sampling is uniform. 
To check consistency of the sample with this model of the tank-capturing process, Goodman \cite{goodman1954some} demonstrates a test of the hypothesis that the sample of serial numbers is from a uniform distribution. 
Interesting extensions of the textbook German tank problem could involve modeling selection bias in the tank-capturing process. 
Such bias could arise e.g.\ hypothetically, if older tanks with smaller serial numbers were more likely to be deployed in the fronts opened earlier in the war, where capturing tanks is more difficult than at less fortified fronts opened more recently. Selection bias could manifest in e.g.\ clusters in the observed serial numbers.
%	\item presuming the serial numbers were inscribed on a \emph{component} of the tank, adjusting (reducing) the estimate of the tank population size by considering the rate at which that component is replaced.  \cite{ruggles1947empirical}
%\end{itemize}

% multiple factories with overlapping serial numbers.
% https://www.biorxiv.org/content/10.1101/2021.07.06.451319v1.full.pdf solved here?
% the Willliams paper?

\paragraph{The German tank problem in other contexts.}
The Bayesian probability theory to solve the German tank problem applies (perhaps, with modification) to many other contexts where we wish to estimate the size of some finite, hidden set \cite{cheng2020estimating}, e.g.: the number of taxicabs in a city \cite{grajalez2013great,comap}, racing cars on a track \cite{tenenbein1971racing}, accounts at a bank \cite{hohle2006bayesian}, furniture pieces purchased by a university \cite{goodman1954some}, aircraft operations at an airport \cite{mott2016estimation}, cases in court \cite{wu2022augmenting}, or electronic devices produced by a company \cite{iphoneguardian}---and the extent of leaked classified government communications \cite{gill2015estimating}, the time needed to complete a project deadline \cite{fehlmann2017new}, the time-coverage of historical records of extreme events like floods \cite{prosdocimi2018german}, 
%the maximum in a large data set \cite{gum2005estimating},
 the length of a short-tandem repeat allele \cite{tang2017profiling}, the size of a social network \cite{katzir2011estimating}, the lifetime of a flower of a plant \cite{pearse2017statistical}, or the duration of existence of a species \cite{roberts2003did}.
Mark and recapture methods in ecology to estimate the size of an animal population \cite{nichols1992capture,chao2001overview} are tangentially related to the German tank problem.

\paragraph{The practice of inscribing sequential serial numbers on military equipment.}
Germany adopted the practice of marking their military equipment with serial numbers and codes to trace the equipment/parts/components back to the manufacturer. However, the sequential nature of these serial numbers was exploited by the Allies to estimate their armament production. 
% However, serial numbers on equipment are advantageous for (i) tracing faulty/defective equipment/parts back to the manufacturer for quality control and (ii) handling of spare parts. \cite{ruggles1947empirical} 
To reduce vulnerability to serial number analysis for estimating production while maintaining advantages of tracing equipment back to the manufacturer, serial numbers and codes could instead be encrypted \cite{delfs2002introduction} or obfuscated by e.g.\ chaffing \cite{rivest1998chaffing}. 

\section*{Data and code availability} The Julia \cite{bezanson2012julia} code to reproduce all of our visualizations drawn using Makie.jl \cite{DanischKrumbiegel2021} is available on Github at \url{github.com/SimonEnsemble/the_German_tank_problem}.

\section*{Acknowledgements}
This work has been supported by US Department of Homeland Security Countering Weapons of Mass Destruction under CWMD Academic Research Initiative Cooperative Agreement \#21CWDARI00043. This support does not constitute an express or implied endorsement on the part of the Government.
Thanks to Bernhard Konrad and Edward Celarier for providing detailed feedback on the manuscript, my students Gbenga Fabusola, Adrian Henle, and Paul Morris for feedback on the introduction, the anonymous reviewer for providing the idea for Sec.~\ref{sec:sims}, and Paul Campbell for informing me about the example of iPhone estimation.

\bibliography{refs}
\bibliographystyle{unsrt}

\end{document}  
