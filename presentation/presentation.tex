\documentclass[10pt]{beamer}

\usetheme{metropolis}

\usepackage{appendixnumberbeamer}
% \includeonlyframes{applications}
\usepackage{multicol}
\usepackage{color}
\usepackage{emoji}

\usepackage{amsmath}
\usepackage{subcaption}
\DeclareMathOperator*{\argmax}{arg\,max}
% dope docs http://mirrors.ibiblio.org/CTAN/macros/latex/contrib/beamer-contrib/themes/metropolis/doc/metropolistheme.pdf#page8
\usepackage[style=phys,citestyle=numeric,backend=bibtex]{biblatex}
% https://tex.stackexchange.com/questions/167665/multiple-references-with-footfullcite to hv multiple footnoterefs
% \renewcommand\multicitedelim{\addsemicolon\space}
\renewcommand{\footnotesize}{\tiny} % change fontsize for footnotes
\addbibresource{../paper/refs.bib}

\definecolor{osuorange}{rgb}{0.84,0.25,0.035}
\setbeamercolor{frametitle}{bg=white, fg=normal text.fg}
\setbeamercolor{title separator}{fg=osuorange, bg=white}
\usepackage{booktabs}

\metroset{subsectionpage=progressbar, sectionpage=progressbar}

\usepackage[font=scriptsize]{caption}
\captionsetup{labelformat=empty, justification=centering,font=scriptsize,labelfont=scriptsize}% redefines the caption setup of the figures environment in the beamer class.
\setbeamerfont{caption}{size=\tiny}        
% \addtobeamertemplate{frametitle}{}{\vspace{-\baselineskip}}
    
% change background color
\setbeamercolor{background canvas}{bg=white}
\newcommand{\data}{$(s_1, ..., s_k)$\xspace}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\title{the German tank problem}
\subtitle{a Bayesian treatment}
% \date{\today}
\author{Cory M. Simon}
\institute{
	Assistant Professor\\
	School of Chemical, Biological, and Environmental Engineering\\
	Oregon State University\\
	\texttt{simonensemble.github.io} \\
	\texttt{@CoryMSimon}
}
\date{}
\titlegraphic{\hfill\includegraphics[height=1.0cm]{logo.png}}

\begin{document}
\maketitle

\begin{frame}[t]{read the preprint!}
	C. Simon. "A Bayesian treatment of the German tank problem." \emph{arXiv}. (2023)
	
	\begin{figure}[h!]
	\centering
 	\includegraphics[width=0.4\textwidth]{../paper/the_sample.pdf}
 	\includegraphics[width=0.4\textwidth]{../paper/all.pdf}
	\end{figure}
	
	feedback welcome.
\end{frame}

\section{historical context}

\begin{frame}[t]{World War II (1939-1945)}
	\begin{figure}
		\centering
		\includegraphics[width=0.85\textwidth]{WWII_deaths.png}
		\caption{source: Wikipedia}
	\end{figure}
\end{frame}

\begin{frame}[t]{estimating Germany's armament production}

	\alert{context:} the Allies wish to estimate Germany's production of tanks, tires, rockets, etc. 
	
	\begin{figure}
		\centering
		\includegraphics[width=0.3\textwidth]{panther_tank.jpg}
		\caption{the German Panther tank. source: Wikipedia}
	\end{figure}

	\pause
	
	conventional methods were unreliable and/or contradictory\footfullcite{ruggles1947empirical}.
	\begin{itemize}
		\item extrapolating prewar manufacturing capabilities
		\item reports from secret sources
		\item interrogating prisoners of war
	\end{itemize}
	
	\vspace{\baselineskip}
\end{frame}

\begin{frame}[t]{Germany's practice of inscribing military equipment with serial numbers}
	Germany marked their military equipment with serial numbers and codes for the date and/or place of manufacture\footfullcite{ruggles1947empirical} to:
	\begin{itemize}
		\item facilitate handling spare parts
		\item trace defective equipment/parts back to the manufacturer for quality control.
	\end{itemize}
	
	\pause 
	
	\emoji{bulb} these markings on a captured sample of German equipment conveyed information to British and American economic intelligence agencies about Germany's production of it.
\end{frame}

\begin{frame}[t]{serial number analysis for German tank production}
	the Allies collected serial numbers on the chassis, engines, gearboxes, and bogie wheels of samples of tanks by inspecting captured tanks and examining captured records \footfullcite{ruggles1947empirical}.
	
	\begin{table}[h!]
	\small
		\centering 
		\caption{Monthly production of tanks by Germany.} 	
\begin{tabular}{p{1.6cm} p{2.5cm} p{2.5cm} p{2cm}}
\toprule
 & \multicolumn{2}{c}{estimates} &   \\ 
\cmidrule(r){2-3}
date & conventional intelligence &  \only<2-3>{serial number analysis}  & \only<3>{German records} \\
\midrule
Jun. 1940 & 1000&  \only<2-3>{169} &  \only<3>{122} \\
Jun. 1941 &1550 &  \only<2-3>{244} &   \only<3>{271} \\
Aug. 1942 & 1550&  \only<2-3>{327}  & \only<3>{342} \\
\bottomrule
\end{tabular}
\end{table}

\end{frame}

\section{the German tank problem}


\begin{frame}[t]{the German tank problem}
\begin{alertblock}{problem statement}
the German military has $n$ tanks. 
each tank is inscribed with a unique serial number in $\{1, ..., n\}$. \newline

as the Allies, we do not know $n$, but we captured (without replacement) a sample of $k$ tanks with serial numbers \data. 

\begin{center}
	\begin{tabular}{cccc}
		\includegraphics[width=0.125\textwidth]{../paper/tank.png} &  \includegraphics[width=0.125\textwidth]{../paper/tank.png}  & & \includegraphics[width=0.125\textwidth]{../paper/tank.png} \\
		\large $s_1$ & \large $s_2$ & \large $\cdots$ & \large $s_k$ 
	\end{tabular}
\end{center}


\alert{objective}: estimate $n$ in consideration of the data \data.

\end{alertblock}
\pause 

\alert{key assumption}: all tanks in the population were equally likely to be captured.
\end{frame}

\begin{frame}[t]{the German tank problem}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[height=0.5\textwidth]{turing.jpg} \caption{Alan Turing}
	\end{subfigure} 
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[height=0.5\textwidth]{gleason.jpg}  \caption{Andrew Gleason}
	\end{subfigure} 
\end{figure}


\begin{exampleblock}{origins?}
In 1942, Alan Turing and Andrew Gleason discussed a variant of the German tank problem, ``how to best to estimate the total number of taxicabs in a town, having seen a random selection of their license numbers'', in a crowded restaurant in Washington DC \footfullcite{hodges2014alan}$^,$\footfullcite{hall2014alan}. 
\end{exampleblock}
\end{frame}

\begin{frame}[t]{example}
\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{../paper/the_sample.pdf}
	\caption{the data $(s_1, s_2, s_3)$.}
\end{figure}

what is your estimate of $n$?
\end{frame}

\begin{frame}[t]{quantifying uncertainty in the tank population size}



 any estimate of the tank population size $n$ from the data \data is subject to uncertainty.

\pause 
quantifying uncertainty in our estimate of $n$ is important because high-stakes military decisions may be made on its basis.

\emoji{t-rex}: "I estimate $n$ to be between 15 and 25 with 80\% confidence."

\end{frame}

\begin{frame}[t]{a Bayesian treatment of the German tank problem}
	\alert{this talk}: the Bayesian approach to the German tank problem 
	\begin{itemize}
		\item solution quantifies uncertainty by assigning a probability to each tank population size
		\pause 
		\item provides an opportunity to incorporate prior information and/or beliefs about the tank population size into the solution.
	\end{itemize}
\end{frame}

\section{prior work on the German tank problem}


\begin{frame}[t]{references}
\alert{the frequentist approach.}
Goodman \footfullcite{goodman1952serial,goodman1954some}, and Clark, Gonye, and Miller \footfullcite{clark2021lessons}. 

\alert{for pedagogy.} Champkin, \footfullcite{grajalez2013great},
Johnson \footfullcite{johnson1994estimating},
Scheaffer, Watkins, Gnanadesikan, and Witmer \footfullcite{scheaffer2013activity}, Berg \footfullcite{berg2021bayesian}.

\alert{the Bayesian approach.}
Roberts \footfullcite{roberts1967informative}, H{\"o}hle and Held \footfullcite{hohle2006bayesian}, and Linden, Dose, and Toussaint \footfullcite{von2014bayesian}, Cocco, Monasson, and Zamponi \footfullcite{cocco2022statistical}, and Andrews \footfullcite{blogpost}.
\end{frame}


\section{a Bayesian treatment of the German tank problem}

\subsection{modeling uncertainty}

\begin{frame}[t]{treat $N$ as a discrete random variable}
a probability mass function (PMF) of $N$ assigns a probability to each possible tank population size $n\in\{0,1,...\}$. 

\pause 

\emoji{bulb} this probability is a measure of our degree of belief, perhaps with some basis in knowledge/data, that the tank population size is $n$. \footfullcite{ghosh2006introduction}

\pause 

the observed serial numbers \data provide information about the tank population size. consequently, $N$ has a:
\begin{itemize}
	\item prior PMF
	\item posterior PMF
\end{itemize}
\end{frame}

\subsection{ingredient 1: the prior PMF}

\begin{frame}[t]{the prior PMF of $N$, $\pi_{\text{prior}}(N=n)$}
expresses a combination of our subjective beliefs and objective knowledge about the total number of tanks $N$ before the data \data are collected and considered. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\textwidth]{../paper/prior.pdf}
\end{figure}

\begin{equation*}
	\pi_{\text{prior}}(N=n) = \dfrac{1}{n_{\max}+1} \mathcal{I}_{ \{0, ..., n_{\text{max}}\}}(n).	 
\end{equation*}
\end{frame}

\subsection{ingredient 2: the data}

\begin{frame}[t]{the data, $s^{(k)}:=(s_1,...,s_k)$}
\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{../paper/the_sample.pdf}
	\caption{the data $s^{(3)}=(15, 14, 3)$.}
\end{figure}

\pause 
\emoji{bulb} $s^{(k)}$ is a realization of the discrete random vector $S^{(k)}:=(S_1, ..., S_k)$. 
\end{frame}

\subsection{ingredient 3: the likelihood function}

\begin{frame}[t]{the likelihood function, $\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)$}
\begin{itemize}
	\item specifies the probability of the data $S^{(k)}=s^{(k)}$ given the tank population size $N=n$
	\item quantifies the support the data $s^{(k)}$ lend for each tank population size $n$
	\item constructed from a probabilistic model of the data-generating process
\end{itemize}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\textwidth]{../paper/likelihood_presentation.pdf}
\end{figure}
\end{frame}


\begin{frame}[t]{the likelihood function, $\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)$}
\alert{a probabilistic model of the data-generating process}: sequential capture of $k$ tanks from a population of $n$ tanks, without replacement.

\pause 

\begin{exampleblock}{perspective 1: a uniform distribution over outcomes}
each outcome $s^{(k)}$ belonging to the sample space
\begin{equation*}
	\Omega_n^{(k)} := \{ (s_1, ..., s_k)_{\neq}  :  s_i \in \{1, ..., n\} \; \text{for all } i \in \{ 1,..., k \} \}
\end{equation*} 
is equally likely.

\pause the number of possible outcomes is
\begin{equation*}
	\lvert \Omega_n^{(k)} \rvert= n(n-1)\cdots (n-(k-1)) = n! / (n-k)! =:(n)_k
\end{equation*}

\pause the DGP is then the uniform distribution
\begin{equation*}
	\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)=
	\dfrac{1}{(n)_k} \mathcal{I}_{\Omega_n^{(k)}}\left(s^{(k)}\right).
\end{equation*}
\end{exampleblock}
\end{frame}


\begin{frame}[t]{the likelihood function, $\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)$}
\alert{a probabilistic model of the data-generating process}: sequential capture of $k$ tanks from a population of $n$ tanks, without replacement.


\begin{exampleblock}{perspective 2: a sequence of events, $S_1=s_1, ..., S_k=s_k$}
\pause 

	\begin{equation*}
		\pi (S_i=s_i \mid N=n, S_1=s_1, ..., S_{i-1}=s_{i-1})=\dfrac{1}{n-(i-1)} \mathcal{I}_{ \{1,...,n\} \setminus \{s_1, ..., s_{i-1}\}}(s_i)
	\end{equation*}

\pause chain rule:
\begin{equation*}
	\pi(S_1=s_1, ..., S_k=s_k \mid N=n) = \displaystyle \prod_{i=1}^k \pi (S_i=s_i \mid N=n, S_1=s_1,...,S_{i-1}=s_{i-1}).
\end{equation*}

\pause ie.
\begin{equation*}
	\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)=
	\dfrac{1}{(n)_k} \mathcal{I}_{\Omega_n^{(k)}}\left(s^{(k)}\right).
\end{equation*}

\end{exampleblock}
\end{frame}

\begin{frame}[t]{the likelihood function, $\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)$}
\begin{equation*}
	\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n)=
	\dfrac{1}{(n)_k} \mathcal{I}_{\Omega_n^{(k)}}\left(s^{(k)}\right)
\end{equation*}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.45\textwidth]{../paper/the_sample.pdf}
	
	\includegraphics[width=0.5\textwidth]{../paper/likelihood_presentation.pdf}
\end{figure}
\end{frame}

\subsection{turning the Bayesian crank to obtain the posterior PMF}

\begin{frame}[t]{the posterior PMF of $N$, $\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)})$}
\emoji{bulb} assigns a probability to each possible tank population size $n$ in consideration of its consistency with (1) the data \data, according to the likelihood, and (2) our prior beliefs/knowledge encoded in $\pi_{\text{prior}}(N=n)$. 

\pause 

\alert{Bayes' theorem}:
\begin{equation*}
	\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)}) = 
	\frac{\pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n) \pi_{\text{prior}}(N=n)}{\pi_{\text{data}}(S^{(k)}=s^{(k)})},
\end{equation*} with
\begin{equation*}
	\pi_{\text{data}}(S^{(k)}=s^{(k)})= \displaystyle \sum_{n^\prime=0}^\infty \pi_{\text{likelihood}}(S^{(k)}=s^{(k)} \mid N=n^\prime) \pi_{\text{prior}}(N=n^\prime). \label{eq:prob_data}
\end{equation*}

\pause 
$\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)})$ is the raw solution to the German tank problem!
\end{frame}

\begin{frame}[t]{the posterior PMF of $N$, $\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)})$}
let 
\begin{equation*}
    m^{(k)} = \max_{i \in \{1, ..., k\}} s_i .
\end{equation*} 

\pause 
\begin{align*}
	\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)}) 
	& =\frac{
		\displaystyle (n)_k^{-1} \pi_{\text{prior}}(N=n)
	}{
		\displaystyle \sum_{n^\prime=m^{(k)}}^\infty (n^\prime)_{k}^{-1}  \pi_{\text{prior}}(N=n^\prime)
	}
	\mathcal{I}_{\{m^{(k)}, m^{(k)}+1,...\}}(n) \\
	&= \pi_{\text{posterior}}(N=n \mid M^{(k)}=m^{(k)})
	\label{eq:post_simple}
\end{align*}

ie. the data $s^{(k)}$ can be distilled to $m^{(k)}$.
\end{frame}


\begin{frame}[t]{the posterior PMF of $N$, $\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)})$}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.45\textwidth]{../paper/the_sample.pdf}
	
	\includegraphics[width=0.6\textwidth]{../paper/all.pdf}	
\end{figure}
\end{frame}


\begin{frame}[t]{summarizing posterior PMF of $N$}

\begin{alertblock}{the $\alpha$-high-mass subset \footfullcite{hyndman1996computing}}
\begin{equation}
	\mathcal{H}_\alpha := \{n^\prime : \pi_{\text{posterior}}(N=n^\prime \mid M^{(k)}=m^{(k)}) \geq \pi_\alpha\}
\end{equation} where $\pi_\alpha$ is the largest mass to satisfy 
\begin{equation}
	\pi_{\text{posterior}}(N \in \mathcal{H}_\alpha \mid M^{(k)}=m^{(k)}) \geq 1 - \alpha.
\end{equation}

\end{alertblock}
$\mathcal{H}_\alpha$ is the smallest subset of $\{0,...\}$ to (i) contain at least a fraction $1-\alpha$ of the posterior mass of $N$ and (ii) ensure every tank population size belonging to it is more probable than any outside of it.
\end{frame}


\begin{frame}[t]{the posterior PMF of $N$, $\pi_{\text{posterior}}(N=n \mid S^{(k)}=s^{(k)})$}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\textwidth]{../paper/posterior_35.pdf}
\end{figure}

median: $19$

high-mass credible subset $\mathcal{H}_{0.2}=\{15, ..., 25\}$

eg. $\pi_{\text{posterior}}(N>30 \mid M^{(3)}=15)\approx 0.066$.
\end{frame}

\subsection{updating our posterior PMF with more data}

\begin{frame}[t]{what if we capture more tanks?}
  \begin{figure}[h!]
        	\centering
        	\includegraphics[width=0.4\textwidth]{../paper/the_big_sample.pdf}
	 \includegraphics[width=0.5\textwidth]{../paper/posterior_big_35.pdf}
\end{figure}
\end{frame}

\section{conclusions}


\begin{frame}[t]{a Bayesian treatment of the German tank problem}
	incorporate prior assumptions, quantify uncertainty about the tank population size, $N$
	
	\begin{figure}[h!]
		\centering
 		\includegraphics[width=0.4\textwidth]{../paper/the_sample.pdf}
 		\includegraphics[width=0.4\textwidth]{../paper/all.pdf}
	\end{figure}
	
	\pause
	
	\alert{limitation}: uncertainty about tank-capturing model neglected. selection bias may be present.
\end{frame}

\begin{frame}[t]{the German tank problem in other contexts}
estimating the size of some finite, ``hidden'' set, eg. the:
\begin{itemize} 
\item number of taxicabs in a city 
\item number of accounts at a bank
\item  number of aircraft operations at an airport
\item  extent of leaked classified government communications
\item  time-coverage of historical records of extreme events like floods
\item  number of iPhones produced by Apple 
\item  length of a short-tandem repeat allele 
\item  size of a social network 
\item  number of cases in court 
\item  lifetime of a flower of a plant 
\item duration of existence of a species 
\end{itemize}

tangentially related: mark and recapture methods in ecology to estimate the size of an animal population. 
\end{frame}

\end{document}
